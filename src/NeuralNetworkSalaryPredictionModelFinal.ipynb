{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dcfa5bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cristallobo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cristallobo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cristallobo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/cristallobo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports necessary for preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# NLP Imports\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "# Code to download corpora\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Imports to create Neural Net and metrics associated\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2c77e338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1200 entries, 0 to 1199\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Unnamed: 0    1200 non-null   int64  \n",
      " 1   Title         1200 non-null   object \n",
      " 2   Company       1200 non-null   object \n",
      " 3   Location      1200 non-null   object \n",
      " 4   Rating        745 non-null    float64\n",
      " 5   Date          1200 non-null   object \n",
      " 6   Salary        582 non-null    object \n",
      " 7   Description   1199 non-null   object \n",
      " 8   Links         1200 non-null   object \n",
      " 9   Descriptions  1200 non-null   object \n",
      "dtypes: float64(1), int64(1), object(8)\n",
      "memory usage: 93.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 582 entries, 1 to 1198\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   Unnamed: 0   582 non-null    int64 \n",
      " 1   Title        582 non-null    object\n",
      " 2   Salary       582 non-null    object\n",
      " 3   Description  582 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 22.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data in with read_csv\n",
    "file_path = \"./data_science_jobs_indeed_usa.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()\n",
    "\n",
    "# check the basic information of the data set, dtypes, null values, column names\n",
    "df.info()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Drop the columns that we won't need, Company, ContractType, ContractTime, SalaryRaw, LocationRaw, ID\n",
    "new_df = df.drop(columns = [\"Company\", \"Rating\", \"Date\", \"Location\", \"Links\", \"Descriptions\"])\n",
    "new_df.head()\n",
    "\n",
    "# check datatypes again\n",
    "new_df.dtypes\n",
    "\n",
    "# drop the null values of this dataframe\n",
    "new_df.dropna(inplace = True)\n",
    "\n",
    "new_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0fce7a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preferred candidates will have prior experience in implementing Cloud-hosted business process migration in Software as a Service (SAAS) implementations‚Ä¶\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'preferred candidate prior experience implementing cloudhosted business process migration software service saas implementation'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a varialbe for wordnetlemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(article):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    sw_addons = {\"k\", \"uk\",\"also\"} \n",
    "    # Substitute everything that is not a letter with an empty string\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    # we sub in an extra character for anything that is not a character from the\n",
    "    # above line of code\n",
    "    re_clean = regex.sub('', article)\n",
    "    # tokenize each word in the sentence\n",
    "    words = word_tokenize(re_clean)\n",
    "    # obtain the root word for each word \n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # obtain an output that is all lowercase and not in the stop words\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw.union(sw_addons)]\n",
    "    output = ' '.join(output)\n",
    "    return output\n",
    "\n",
    "# test function on sliced df to make sure it is correct\n",
    "print(new_df[\"Description\"][1])\n",
    "clean_text(new_df[\"Description\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d5688af2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Salary</th>\n",
       "      <th>Description</th>\n",
       "      <th>CleanDescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>$80 - $120 an hour</td>\n",
       "      <td>Preferred candidates will have prior experienc...</td>\n",
       "      <td>preferred candidate prior experience implement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>$90,000 - $110,000 a year</td>\n",
       "      <td>Incorporate core data management competencies ...</td>\n",
       "      <td>incorporate core data management competency in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Network Administrator/dba developer</td>\n",
       "      <td>$50,000 - $70,000 a year</td>\n",
       "      <td>The Network Administrator provides 2nd level e...</td>\n",
       "      <td>network administrator provides nd level enduse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Senior Manager-Data Science</td>\n",
       "      <td>$75 - $90 an hour</td>\n",
       "      <td>Stay aware of emerging data science techniques...</td>\n",
       "      <td>stay aware emerging data science technique tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>From $50 an hour</td>\n",
       "      <td>Should have strong data analysis.\\nProven expe...</td>\n",
       "      <td>strong data analysisproven experience design i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                Title  \\\n",
       "1            1                     Business Analyst   \n",
       "3            3                        Data Engineer   \n",
       "4            4  Network Administrator/dba developer   \n",
       "8            8          Senior Manager-Data Science   \n",
       "10          10                        Data Engineer   \n",
       "\n",
       "                       Salary  \\\n",
       "1          $80 - $120 an hour   \n",
       "3   $90,000 - $110,000 a year   \n",
       "4    $50,000 - $70,000 a year   \n",
       "8           $75 - $90 an hour   \n",
       "10           From $50 an hour   \n",
       "\n",
       "                                          Description  \\\n",
       "1   Preferred candidates will have prior experienc...   \n",
       "3   Incorporate core data management competencies ...   \n",
       "4   The Network Administrator provides 2nd level e...   \n",
       "8   Stay aware of emerging data science techniques...   \n",
       "10  Should have strong data analysis.\\nProven expe...   \n",
       "\n",
       "                                     CleanDescription  \n",
       "1   preferred candidate prior experience implement...  \n",
       "3   incorporate core data management competency in...  \n",
       "4   network administrator provides nd level enduse...  \n",
       "8   stay aware emerging data science technique tec...  \n",
       "10  strong data analysisproven experience design i...  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new column that has the clean description of the job\n",
    "new_df['CleanDescription'] = new_df['Description'].apply(clean_text)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "58d646d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>data</th>\n",
       "      <th>database</th>\n",
       "      <th>experience</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business  data  database  experience  learning  machine  year\n",
       "0         1     0         0           1         0        0     0\n",
       "1         0     5         0           0         0        0     0\n",
       "2         0     0         0           0         0        0     0\n",
       "3         0     2         0           0         0        0     0\n",
       "4         0     2         0           1         0        0     0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the COUNT for the working corpus.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=.12)\n",
    "count_vectorizer = vectorizer.fit_transform(new_df[\"CleanDescription\"])\n",
    "words_df = pd.DataFrame(count_vectorizer.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "words_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a42c0f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business</th>\n",
       "      <th>data</th>\n",
       "      <th>database</th>\n",
       "      <th>experience</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   business  data  database  experience  learning  machine  year\n",
       "0         1     0         0           1         0        0     0\n",
       "1         0     1         0           0         0        0     0\n",
       "2         0     0         0           0         0        0     0\n",
       "3         0     1         0           0         0        0     0\n",
       "4         0     1         0           1         0        0     0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since there are words that were missed with the stop words, I want to give each word an equal weight of one. I want to do this because\n",
    "# I do not want the word \"said\" to out way the word \"engineer\" as an example\n",
    "# Filter the dataframe so each word has a weight of 1 \n",
    "filtered_df_2 = words_df.replace(list(range(1,100)),1)\n",
    "filtered_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "76efca20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Salary</th>\n",
       "      <th>business</th>\n",
       "      <th>data</th>\n",
       "      <th>database</th>\n",
       "      <th>experience</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business Analyst</td>\n",
       "      <td>$80 - $120 an hour</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>$90,000 - $110,000 a year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Network Administrator/dba developer</td>\n",
       "      <td>$50,000 - $70,000 a year</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior Manager-Data Science</td>\n",
       "      <td>$75 - $90 an hour</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Engineer</td>\n",
       "      <td>From $50 an hour</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title                     Salary  business  \\\n",
       "1                      Business Analyst         $80 - $120 an hour       0.0   \n",
       "3                         Data Engineer  $90,000 - $110,000 a year       0.0   \n",
       "4   Network Administrator/dba developer   $50,000 - $70,000 a year       0.0   \n",
       "8           Senior Manager-Data Science          $75 - $90 an hour       0.0   \n",
       "10                        Data Engineer           From $50 an hour       0.0   \n",
       "\n",
       "    data  database  experience  learning  machine  year  \n",
       "1    1.0       0.0         0.0       0.0      0.0   0.0  \n",
       "3    1.0       0.0         0.0       0.0      0.0   0.0  \n",
       "4    1.0       0.0         1.0       0.0      0.0   0.0  \n",
       "8    1.0       0.0         0.0       0.0      0.0   0.0  \n",
       "10   1.0       0.0         1.0       1.0      1.0   0.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# combine the two dataframes\n",
    "combined_df = pd.concat([new_df, filtered_df_2], axis = 1)\n",
    "\n",
    "# drop the null values from the new dataframe\n",
    "combined_df.dropna(inplace = True)\n",
    "\n",
    "# drop the 2 description columns as we no longer need them\n",
    "combined_df = combined_df.drop(columns = [\"Description\", \"CleanDescription\",\"Unnamed: 0\"])\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0cd2613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n",
      "45.0\n",
      "25.0\n",
      "82.5\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def normalize_salary(salary_str):\n",
    "    \"\"\"\n",
    "    Normalizes salary string to hourly rate in USD.\n",
    "    \"\"\"\n",
    "    \n",
    "    salary_str = salary_str.lower().strip()\n",
    "    if 'hour' in salary_str:\n",
    "        # hourly rate\n",
    "        rate_range = re.findall('\\d+\\.?\\d*', salary_str)\n",
    "        low_rate = float(rate_range[0])\n",
    "        high_rate = float(rate_range[-1])\n",
    "        avg_rate = (low_rate + high_rate) / 2\n",
    "        #rate = float(re.search('\\d+\\.?\\d*', salary_str).group(0))\n",
    "        return avg_rate\n",
    "    else:\n",
    "        # annual rate\n",
    "        rate_range = re.findall('\\d+\\.?\\d*', salary_str)\n",
    "        low_rate = float(rate_range[0].replace(',', ''))\n",
    "        high_rate = float(rate_range[-1].replace(',', ''))\n",
    "        avg_rate = (low_rate + high_rate) / 2\n",
    "        # convert annual rate to hourly rate\n",
    "        #avg_rate /= 52\n",
    "        #avg_rate /= 40\n",
    "        return avg_rate\n",
    "\n",
    "# Example usage:\n",
    "print(normalize_salary('80-120 an hour'))  # Output: 100.0\n",
    "print(normalize_salary('90,000-110,000 a year'))  # Output: 42.30769230769231\n",
    "print(normalize_salary('50,000-70,000 a year'))  # Output: 21.634615384615383\n",
    "print(normalize_salary('75-90 an hour'))  # Output: 82.5\n",
    "print(normalize_salary('From $50 an hour'))  # Output: 50.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "064c05f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  Salary  business  data  \\\n",
      "1                                   Business Analyst   100.0       0.0   1.0   \n",
      "3                                      Data Engineer    45.0       0.0   1.0   \n",
      "4                Network Administrator/dba developer    25.0       0.0   1.0   \n",
      "8                        Senior Manager-Data Science    82.5       0.0   1.0   \n",
      "10                                     Data Engineer    50.0       0.0   1.0   \n",
      "..                                               ...     ...       ...   ...   \n",
      "572           Senior Business Intelligence Developer    42.5       0.0   0.0   \n",
      "574                                 Business Analyst   417.0       0.0   1.0   \n",
      "575                                    Data Engineer    54.0       0.0   1.0   \n",
      "577                      Manager of Data Engineering    82.5       1.0   1.0   \n",
      "581  Director, Team Lead - Data & Analytics Delivery   304.0       0.0   1.0   \n",
      "\n",
      "     database  experience  learning  machine  year  \n",
      "1         0.0         0.0       0.0      0.0   0.0  \n",
      "3         0.0         0.0       0.0      0.0   0.0  \n",
      "4         0.0         1.0       0.0      0.0   0.0  \n",
      "8         0.0         0.0       0.0      0.0   0.0  \n",
      "10        0.0         1.0       1.0      1.0   0.0  \n",
      "..        ...         ...       ...      ...   ...  \n",
      "572       1.0         0.0       0.0      0.0   0.0  \n",
      "574       0.0         0.0       0.0      0.0   0.0  \n",
      "575       0.0         0.0       0.0      0.0   0.0  \n",
      "577       0.0         1.0       0.0      0.0   1.0  \n",
      "581       0.0         0.0       0.0      0.0   0.0  \n",
      "\n",
      "[286 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "combined_df['Salary'] = combined_df['Salary'].astype(str).apply(normalize_salary)\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0700569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Salary  business  data  database  experience  learning  machine  year  \\\n",
      "1     100.0       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "3      45.0       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "4      25.0       0.0   1.0       0.0         1.0       0.0      0.0   0.0   \n",
      "8      82.5       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "10     50.0       0.0   1.0       0.0         1.0       1.0      1.0   0.0   \n",
      "..      ...       ...   ...       ...         ...       ...      ...   ...   \n",
      "572    42.5       0.0   0.0       1.0         0.0       0.0      0.0   0.0   \n",
      "574   417.0       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "575    54.0       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "577    82.5       1.0   1.0       0.0         1.0       0.0      0.0   1.0   \n",
      "581   304.0       0.0   1.0       0.0         0.0       0.0      0.0   0.0   \n",
      "\n",
      "     Title_AI Data Annotation & Deployment Manager  \\\n",
      "1                                                0   \n",
      "3                                                0   \n",
      "4                                                0   \n",
      "8                                                0   \n",
      "10                                               0   \n",
      "..                                             ...   \n",
      "572                                              0   \n",
      "574                                              0   \n",
      "575                                              0   \n",
      "577                                              0   \n",
      "581                                              0   \n",
      "\n",
      "     Title_AI Python Developer - Entry level  ...  \\\n",
      "1                                          0  ...   \n",
      "3                                          0  ...   \n",
      "4                                          0  ...   \n",
      "8                                          0  ...   \n",
      "10                                         0  ...   \n",
      "..                                       ...  ...   \n",
      "572                                        0  ...   \n",
      "574                                        0  ...   \n",
      "575                                        0  ...   \n",
      "577                                        0  ...   \n",
      "581                                        0  ...   \n",
      "\n",
      "     Title_Sr. Database Administrator (DBA)  \\\n",
      "1                                         0   \n",
      "3                                         0   \n",
      "4                                         0   \n",
      "8                                         0   \n",
      "10                                        0   \n",
      "..                                      ...   \n",
      "572                                       0   \n",
      "574                                       0   \n",
      "575                                       0   \n",
      "577                                       0   \n",
      "581                                       0   \n",
      "\n",
      "     Title_Sr. Director, Data Science, Engineering and Analytics  \\\n",
      "1                                                    0             \n",
      "3                                                    0             \n",
      "4                                                    0             \n",
      "8                                                    0             \n",
      "10                                                   0             \n",
      "..                                                 ...             \n",
      "572                                                  0             \n",
      "574                                                  0             \n",
      "575                                                  0             \n",
      "577                                                  0             \n",
      "581                                                  0             \n",
      "\n",
      "     Title_Sr. Machine Learning Engineer  \\\n",
      "1                                      0   \n",
      "3                                      0   \n",
      "4                                      0   \n",
      "8                                      0   \n",
      "10                                     0   \n",
      "..                                   ...   \n",
      "572                                    0   \n",
      "574                                    0   \n",
      "575                                    0   \n",
      "577                                    0   \n",
      "581                                    0   \n",
      "\n",
      "     Title_Sr. Manager Data Science & Analytics  \\\n",
      "1                                             0   \n",
      "3                                             0   \n",
      "4                                             0   \n",
      "8                                             0   \n",
      "10                                            0   \n",
      "..                                          ...   \n",
      "572                                           0   \n",
      "574                                           0   \n",
      "575                                           0   \n",
      "577                                           0   \n",
      "581                                           0   \n",
      "\n",
      "     Title_Sr. Manager, Data Engineer  Title_Sr. Manager, Data Science  \\\n",
      "1                                   0                                0   \n",
      "3                                   0                                0   \n",
      "4                                   0                                0   \n",
      "8                                   0                                0   \n",
      "10                                  0                                0   \n",
      "..                                ...                              ...   \n",
      "572                                 0                                0   \n",
      "574                                 0                                0   \n",
      "575                                 0                                0   \n",
      "577                                 0                                0   \n",
      "581                                 0                                0   \n",
      "\n",
      "     Title_Staff Database Administrator  Title_Tableau Developer  \\\n",
      "1                                     0                        0   \n",
      "3                                     0                        0   \n",
      "4                                     0                        0   \n",
      "8                                     0                        0   \n",
      "10                                    0                        0   \n",
      "..                                  ...                      ...   \n",
      "572                                   0                        0   \n",
      "574                                   0                        0   \n",
      "575                                   0                        0   \n",
      "577                                   0                        0   \n",
      "581                                   0                        0   \n",
      "\n",
      "     Title_Web Business Analyst  Title_data engineering manager  \n",
      "1                             0                               0  \n",
      "3                             0                               0  \n",
      "4                             0                               0  \n",
      "8                             0                               0  \n",
      "10                            0                               0  \n",
      "..                          ...                             ...  \n",
      "572                           0                               0  \n",
      "574                           0                               0  \n",
      "575                           0                               0  \n",
      "577                           0                               0  \n",
      "581                           0                               0  \n",
      "\n",
      "[286 rows x 156 columns]\n"
     ]
    }
   ],
   "source": [
    "# use get dummies to turn the category columns into number columns\n",
    "encoded_df = pd.get_dummies(combined_df)\n",
    "print(encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c6617126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into X and y\n",
    "X = encoded_df.drop(columns = [\"Salary\"])\n",
    "y = encoded_df[\"Salary\"].values.reshape(-1,1)\n",
    "\n",
    "# look at the shape of each data set\n",
    "X.shape\n",
    "y.shape\n",
    "\n",
    "# lets import train test split to split the data up\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                   y,\n",
    "                                                   random_state=78)\n",
    "# use MinMaxScaler to scale the date\n",
    "x_scaler = MinMaxScaler()\n",
    "y_scaler = MinMaxScaler()\n",
    "\n",
    "# scale the training data\n",
    "x_scaler.fit(X_train)\n",
    "y_scaler.fit(y_train)\n",
    "X_train_scaled = x_scaler.transform(X_train)\n",
    "X_test_scaled = x_scaler.transform(X_test)\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled =y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48368d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "745249b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salary_df = pd.read_csv(\"salary_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bed20bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salary_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f35fca30",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################### Neural Network Model #################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4d75c12d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.6485 - mean_absolute_error: 0.6485 - val_loss: 0.9207 - val_mean_absolute_error: 0.9207 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4727 - mean_absolute_error: 0.4727 - val_loss: 0.9248 - val_mean_absolute_error: 0.9248 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.4264 - mean_absolute_error: 0.4264 - val_loss: 0.9101 - val_mean_absolute_error: 0.9101 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4172 - mean_absolute_error: 0.4172 - val_loss: 0.9329 - val_mean_absolute_error: 0.9329 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.4045 - mean_absolute_error: 0.4045 - val_loss: 0.9131 - val_mean_absolute_error: 0.9131 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3817 - mean_absolute_error: 0.3817 - val_loss: 0.9096 - val_mean_absolute_error: 0.9096 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3192 - mean_absolute_error: 0.3192 - val_loss: 0.9490 - val_mean_absolute_error: 0.9490 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.3177 - mean_absolute_error: 0.3177 - val_loss: 0.9691 - val_mean_absolute_error: 0.9691 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3750 - mean_absolute_error: 0.3750 - val_loss: 0.9199 - val_mean_absolute_error: 0.9199 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3297 - mean_absolute_error: 0.3297 - val_loss: 0.8935 - val_mean_absolute_error: 0.8935 - lr: 0.0500\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.3041 - mean_absolute_error: 0.3041 - val_loss: 0.9038 - val_mean_absolute_error: 0.9038 - lr: 0.0500\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2909 - mean_absolute_error: 0.2909 - val_loss: 0.9080 - val_mean_absolute_error: 0.9080 - lr: 0.0500\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2861 - mean_absolute_error: 0.2861 - val_loss: 0.9058 - val_mean_absolute_error: 0.9058 - lr: 0.0500\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2556 - mean_absolute_error: 0.2556 - val_loss: 0.9045 - val_mean_absolute_error: 0.9045 - lr: 0.0500\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 0.2637 - mean_absolute_error: 0.2637 - val_loss: 0.8921 - val_mean_absolute_error: 0.8921 - lr: 0.0500\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2620 - mean_absolute_error: 0.2620 - val_loss: 0.8903 - val_mean_absolute_error: 0.8903 - lr: 0.0500\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2537 - mean_absolute_error: 0.2537 - val_loss: 0.8739 - val_mean_absolute_error: 0.8739 - lr: 0.0500\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2527 - mean_absolute_error: 0.2527 - val_loss: 0.8816 - val_mean_absolute_error: 0.8816 - lr: 0.0500\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2480 - mean_absolute_error: 0.2480 - val_loss: 0.8977 - val_mean_absolute_error: 0.8977 - lr: 0.0500\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2302 - mean_absolute_error: 0.2302 - val_loss: 0.8893 - val_mean_absolute_error: 0.8893 - lr: 0.0250\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2448 - mean_absolute_error: 0.2448 - val_loss: 0.8983 - val_mean_absolute_error: 0.8983 - lr: 0.0250\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2124 - mean_absolute_error: 0.2124 - val_loss: 0.9092 - val_mean_absolute_error: 0.9092 - lr: 0.0250\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2114 - mean_absolute_error: 0.2114 - val_loss: 0.9065 - val_mean_absolute_error: 0.9065 - lr: 0.0250\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2091 - mean_absolute_error: 0.2091 - val_loss: 0.9023 - val_mean_absolute_error: 0.9023 - lr: 0.0250\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2126 - mean_absolute_error: 0.2126 - val_loss: 0.9030 - val_mean_absolute_error: 0.9030 - lr: 0.0250\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 7ms/step - loss: 0.2062 - mean_absolute_error: 0.2062 - val_loss: 0.9059 - val_mean_absolute_error: 0.9059 - lr: 0.0250\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 8ms/step - loss: 0.2042 - mean_absolute_error: 0.2042 - val_loss: 0.8948 - val_mean_absolute_error: 0.8948 - lr: 0.0250\n",
      "Epoch 27: early stopping\n",
      "Train MAE: 0.33\n",
      "Test MAE: 0.40\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "Mean absolute error: 56.86\n",
      "R-squared: -0.19\n",
      "Correlation coefficient: -0.01\n",
      "     Unnamed: 0                   Title                      Salary  \\\n",
      "134         134  Director, Data Science  $150,000 - $225,000 a year   \n",
      "\n",
      "                                           Description  \\\n",
      "134  Experience in developing advanced data science...   \n",
      "\n",
      "                                      CleanDescription  \n",
      "134  experience developing advanced data science ml...  \n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "   Real  Predicted\n",
      "0  45.0  44.569637\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_value = 0\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# Load data\n",
    "#salary_data = pd.read_csv(\"https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/salary_data.csv\")\n",
    "\n",
    "# Preprocess data\n",
    "#X = salary_data.iloc[:, :-1].values\n",
    "#y = salary_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_value)\n",
    "\n",
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "\n",
    "# Define the neural network architecture\n",
    "number_input_columns = X.shape[1]\n",
    "number_hidden_nodes = X.shape[1] * 2\n",
    "\n",
    "neural_network = Sequential()\n",
    "\n",
    "neural_network.add(Dense(units=number_input_columns, input_dim=number_input_columns, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dropout(0.5))\n",
    "neural_network.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Define the learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "opt = SGD(learning_rate=0.01, momentum=0.8)\n",
    "\n",
    "# Compile the model\n",
    "neural_network.compile(loss=\"mean_absolute_error\", optimizer=opt, metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Define the callbacks\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "callbacks_list = [lrate, es]\n",
    "\n",
    "# Train the model\n",
    "nn_model = neural_network.fit(X_train_scaled, y_train_scaled, validation_split=0.2, epochs=100, batch_size=28, callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_mae = neural_network.evaluate(X_train_scaled, y_train_scaled, verbose=0)\n",
    "test_loss, test_mae = neural_network.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_mae:.2f}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n",
    "\n",
    "# Make predictions on X_test_scaled\n",
    "predictions = neural_network.predict(X_test_scaled)\n",
    "predicted_salaries = y_scaler.inverse_transform(predictions)\n",
    "real_salaries = y_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Evaluate the model using R-squared and correlation coefficient\n",
    "r2 = np.corrcoef(predicted_salaries.ravel(), real_salaries.ravel())\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# make predictions on X_test_scaled\n",
    "predictions = neural_network.predict(X_test_scaled)\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    \"Real\": real_salaries.ravel(),\n",
    "    \"Predicted\": predicted_salaries.ravel()\n",
    "})\n",
    "#print(salaries)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(real_salaries, predicted_salaries)\n",
    "mae = mean_absolute_error(real_salaries, predicted_salaries)\n",
    "r2 = r2_score(real_salaries, predicted_salaries)\n",
    "corr_coef = np.corrcoef(real_salaries.ravel(), predicted_salaries.ravel())[0, 1]\n",
    "\n",
    "#print(\"Mean squared error: {:.2f}\".format(mse))\n",
    "print(\"Mean absolute error: {:.2f}\".format(mae))\n",
    "print(\"R-squared: {:.2f}\".format(r2))\n",
    "print(\"Correlation coefficient: {:.2f}\".format(corr_coef))\n",
    "\n",
    "#### test sample values\n",
    "###\n",
    "print(new_df.iloc[65:66])\n",
    "X_train_sample = X_train.iloc[65:66]\n",
    "y_train_sample = y_train[65:66]\n",
    "#print(X_train_sample)\n",
    "#print(y_train_sample)\n",
    "X_scaled_sample = X_scaler.fit_transform(X_train_sample)\n",
    "y_scaled_sample  = y_scaler.fit_transform(y_train_sample)\n",
    "###\n",
    "prediction_sample = neural_network.predict(X_scaled_sample)\n",
    "\n",
    "predicted_salary_sample = y_scaler.inverse_transform(prediction_sample)\n",
    "real_salary_sample = y_scaler.inverse_transform(y_scaled_sample)\n",
    "salary_sample = pd.DataFrame({\n",
    "    \"Real\": real_salary_sample.ravel(),\n",
    "    \"Predicted\": predicted_salary_sample.ravel()\n",
    "})\n",
    "print(salary_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374bcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8e597773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 0.7340 - mean_absolute_error: 0.7340 - val_loss: 0.5222 - val_mean_absolute_error: 0.5222 - lr: 0.1000\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5411 - mean_absolute_error: 0.5411 - val_loss: 0.5129 - val_mean_absolute_error: 0.5129 - lr: 0.1000\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4874 - mean_absolute_error: 0.4874 - val_loss: 0.5122 - val_mean_absolute_error: 0.5122 - lr: 0.1000\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4770 - mean_absolute_error: 0.4770 - val_loss: 0.5201 - val_mean_absolute_error: 0.5201 - lr: 0.1000\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4496 - mean_absolute_error: 0.4496 - val_loss: 0.5626 - val_mean_absolute_error: 0.5626 - lr: 0.1000\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3905 - mean_absolute_error: 0.3905 - val_loss: 0.6738 - val_mean_absolute_error: 0.6738 - lr: 0.1000\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4614 - mean_absolute_error: 0.4614 - val_loss: 0.5554 - val_mean_absolute_error: 0.5554 - lr: 0.1000\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.4078 - mean_absolute_error: 0.4078 - val_loss: 0.5912 - val_mean_absolute_error: 0.5912 - lr: 0.1000\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3310 - mean_absolute_error: 0.3310 - val_loss: 0.5564 - val_mean_absolute_error: 0.5564 - lr: 0.1000\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3750 - mean_absolute_error: 0.3750 - val_loss: 0.5664 - val_mean_absolute_error: 0.5664 - lr: 0.0500\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3740 - mean_absolute_error: 0.3740 - val_loss: 0.5387 - val_mean_absolute_error: 0.5387 - lr: 0.0500\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3339 - mean_absolute_error: 0.3339 - val_loss: 0.5680 - val_mean_absolute_error: 0.5680 - lr: 0.0500\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3085 - mean_absolute_error: 0.3085 - val_loss: 0.6117 - val_mean_absolute_error: 0.6117 - lr: 0.0500\n",
      "Epoch 13: early stopping\n",
      "Train MAE: 0.33\n",
      "Test MAE: 0.58\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "      Real   Predicted\n",
      "0   390.00  282.962952\n",
      "1    47.50   75.963806\n",
      "2    62.50   55.525646\n",
      "3    41.00  119.779243\n",
      "4   386.50   57.057747\n",
      "5    90.00   60.365604\n",
      "6    54.00   56.468586\n",
      "7    47.00   61.247841\n",
      "8    65.00   94.369225\n",
      "9    60.00   53.782459\n",
      "10   70.00   57.930050\n",
      "11   80.00   84.190987\n",
      "12   70.00   65.539917\n",
      "13   62.50   61.549480\n",
      "14  172.50   61.290668\n",
      "15   50.00   57.906506\n",
      "16  250.50   65.539917\n",
      "17  230.00   54.475750\n",
      "18   30.00   58.631615\n",
      "19   90.00   62.469444\n",
      "20   67.50   62.558018\n",
      "21  391.50   61.839989\n",
      "22   29.00   52.123669\n",
      "23   35.00   50.737148\n",
      "24   45.00   54.776745\n",
      "25  154.50   57.363373\n",
      "26   55.00   53.838581\n",
      "27   50.00   61.856682\n",
      "28   37.50   66.607735\n",
      "29  410.50   57.642387\n",
      "30  389.00   58.385674\n",
      "31   47.16   59.936432\n",
      "32   80.00   71.790359\n",
      "33   50.00   58.212376\n",
      "34   75.00   81.068375\n",
      "35   70.00   56.360161\n",
      "36   80.00   86.031677\n",
      "37  303.00   62.139961\n",
      "38   46.50   70.142357\n",
      "39   75.00   57.043541\n",
      "40  411.50   60.766224\n",
      "41  508.00   57.708652\n",
      "42   35.00   55.529659\n",
      "43   60.00   63.275539\n",
      "44  417.00   64.610512\n",
      "45   54.94   63.387295\n",
      "46   87.50   70.378418\n",
      "47   63.75   58.434547\n",
      "48   62.50   57.500439\n",
      "49   72.50   58.645660\n",
      "50  562.50  555.396606\n",
      "51   65.00   57.581543\n",
      "52   60.00   56.468586\n",
      "53   75.00   53.210869\n",
      "54  458.00   54.513626\n",
      "55   70.00   56.079823\n",
      "56   29.00   57.042603\n",
      "57   80.00   53.782459\n",
      "Mean squared error: 21337.60\n",
      "Mean absolute error: 77.10\n",
      "R-squared: -0.01\n",
      "Correlation coefficient: 0.43\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "#salary_data = pd.read_csv(\"salary_data.csv\")\n",
    "\n",
    "# Split the dataset into input features (X) and target variable (y)\n",
    "#X = salary_data.iloc[:, :-1].values\n",
    "#y = salary_data.iloc[:, -1].values.reshape(-1, 1)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "seed_value = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed_value)\n",
    "\n",
    "# Scale the input features and target variable\n",
    "X_scaler = StandardScaler()\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "\n",
    "# Define the neural network architecture\n",
    "number_input_columns = X.shape[1]\n",
    "number_hidden_nodes = X.shape[1] * 2\n",
    "\n",
    "neural_network = Sequential()\n",
    "\n",
    "neural_network.add(Dense(units=number_input_columns, input_dim=number_input_columns, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dense(units=number_hidden_nodes, activation=\"relu\"))\n",
    "neural_network.add(Dropout(0.5))\n",
    "neural_network.add(Dense(units=1, activation=\"linear\"))\n",
    "\n",
    "# Define the learning rate schedule\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.1\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "# Define the optimizer and compile the model\n",
    "opt = SGD(learning_rate=0.01, momentum=0.8)\n",
    "neural_network.compile(loss=\"mean_absolute_error\", optimizer=opt, metrics=[\"mean_absolute_error\"])\n",
    "\n",
    "# Define the callbacks for the learning rate schedule and early stopping\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "callbacks_list = [lrate, es]\n",
    "\n",
    "# Train the model\n",
    "nn_model = neural_network.fit(X_train_scaled, y_train_scaled, validation_split=0.2, epochs=100, batch_size=28, callbacks=callbacks_list)\n",
    "\n",
    "# Evaluate the model on the training and testing sets\n",
    "train_loss, train_mae = neural_network.evaluate(X_train_scaled, y_train_scaled, verbose=0)\n",
    "test_loss, test_mae = neural_network.evaluate(X_test_scaled, y_test_scaled, verbose=0)\n",
    "\n",
    "print(f\"Train MAE: {train_mae:.2f}\")\n",
    "print(f\"Test MAE: {test_mae:.2f}\")\n",
    "\n",
    "# Make predictions on the testing set and inverse transform the scaled predictions and target variable\n",
    "predictions = neural_network.predict(X_test_scaled)\n",
    "predicted_salaries = y_scaler.inverse_transform(predictions)\n",
    "real_salaries = y_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "# Print the\n",
    "# Evaluate the model using R-squared and correlation coefficient\n",
    "r2 = np.corrcoef(predicted_salaries.ravel(), real_salaries.ravel())\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# make predictions on X_test_scaled\n",
    "predictions = neural_network.predict(X_test_scaled)\n",
    "\n",
    "salaries = pd.DataFrame({\n",
    "    \"Real\": real_salaries.ravel(),\n",
    "    \"Predicted\": predicted_salaries.ravel()\n",
    "})\n",
    "print(salaries)\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(real_salaries, predicted_salaries)\n",
    "mae = mean_absolute_error(real_salaries, predicted_salaries)\n",
    "r2 = r2_score(real_salaries, predicted_salaries)\n",
    "corr_coef = np.corrcoef(real_salaries.ravel(), predicted_salaries.ravel())[0, 1]\n",
    "\n",
    "print(\"Mean squared error: {:.2f}\".format(mse))\n",
    "print(\"Mean absolute error: {:.2f}\".format(mae))\n",
    "print(\"R-squared: {:.2f}\".format(r2))\n",
    "print(\"Correlation coefficient: {:.2f}\".format(corr_coef))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515ad51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
